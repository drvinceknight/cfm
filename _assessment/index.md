---
layout: default
tag: assessment
---

# Individual tests (20%)

A series of 6 individual tests:

- Test 1: [Algebra]({{ site.baseurl }}/topics/algebra.html) and [Calculus]({{ site.baseurl }}/topics/calculus.html) (released Week 3 of Autumn Semester)
- Test 2: [Combinatorics]({{ site.baseurl }}/topics/combinatorics.html) and [Probability]({{ site.baseurl }}/topics/probability.html) (released Week 5 of Autumn Semester).
- Test 3: [Matrices]({{ site.baseurl }}/topics/matrices.html) and [Sequences]({{ site.baseurl }}/topics/sequences.html) (released Week 7 of Autumn Semester).
- Test 4: [Statistics]({{ site.baseurl }}/topics/statistics.html) and [Differential Equations]({{ site.baseurl }}/topics/differential-equations.html) (released Week 9 of Autumn Semester).
- Test 5: [Variables, conditional statements and while loops]({{ site.baseurl }}/topics/variables-conditional-statements-and-while-loops.html) and [Functions and
  data structures]({{ site.baseurl }}/topics/functions-and-data-structures.html) (released Week 2 of Spring Semester).
- Test 6: [Object Oriented Programming]({{ site.baseurl }}/topics/object-oriented-programming.html) and [Using a terminal and an
  editor]({{ site.baseurl }}/topics/using-a-terminal-and-an-editor.html) (released Week 4 of Spring Semester).

You can complete each test when it is released. The deadline for completing all
of them is at the end of the Spring Semester.

**I recommend you do each quiz as the corresponding content is covered.**
Content for the first 4 tests is covered in the Autumn Semester. Content for the
last 2 tests is covered in the Spring Semester.

Each quiz corresponds to topics covered in the course. You can attempt each quiz
as many times as you wish (your last attempt is the one that counts).

# Individual coursework (20%)

A mock coursework is available:
[here]({{site.baseurl}}/assets/assessment/mock/mock.ipynb).

Solutions to the mock are [here]({{site.baseurl}}/assets/assessment/mock/mock_solutions.ipynb).

## [Marking criteria](#marking-criteria-individual)

There will be 8 questions, each worth 5 marks:

+ **3 marks** are awarded for the quality of the **analysis**: This means the correctness and the appropriateness of the code.
  - *0 marks*: Code that does not answer the question
  - *1 marks*: Code that partly answers the question correctly
  - *2 marks*: Code that mostly answers the question correctly
  - *3 marks*: Code that answers the question correctly
+ **2 marks** are awarded for the quality of the **communication**: This means the narrative and story given by words (if appropriate for the question), and the clarity of the code including using sensible variable names.
  - *0 marks*: No narrative given
  - *1 marks*: An attempt at some narrative and readable code
  - *2 marks*: Clear narrative, clear code with descriptive variable names


The coursework for 2025/2026 can be downloaded available [here]({{site.baseurl}}/assets/assessment/ind/assignment.ipynb).



# Group project (60%)

Build a Python library to provide tools for a problem related to mathematics.

You will evidence your progress with 2 mediums:

2. A 2 page paper
3. A 15 minute recorded presentation

Your final submission should include the following **7** files:

1. A `main.tex` file: the source file for a 2 page paper written in [LaTeX](({{ site.baseurl }}/topics/latex.html)
   ).
2. A `main.pdf` file: the pdf file for a 2 page paper written in [LaTeX](({{ site.baseurl }}/topics/latex.html)
   ).
3. A `<library>.py` file: the [source file for your Python library]({{ site.baseurl }}/topics/modularisation-of-code.html)
4. A `test_<library>.py` file: [the test files for your Python library]({{ site.baseurl }}/topics/testing-of-code.html)
5. A `README.md` file: [the documentation for your Python library]({{ site.baseurl }}/topics/documentation-of-code.html)
6. A `presentation.mp4` (or similar file format): the video recording of [your 15 minute presentation]({{ site.baseurl }}/topics/presenting-mathematics.html)
7. A `contribution.md` file: a file describing the contributions of every member of your group.

## [Marking criteria](#marking-criteria)

The various components of the submission should aim to demonstrate how the following
aspects of the work have been addressed:

### Communication (both paper and presentation) (30%)

- **Summary**: Has a clear description of the high-level functionality and
  purpose of the software for a diverse, non-specialist audience been
  provided?
- **A statement of need**: Do the authors clearly state what problems the
  software is designed to solve and who the target audience is?
- **Quality of writing**: Is the paper well written (i.e., it does not
  require editing for structure, language, or writing quality)?
- **Functionality**: Have the functional claims of the software been
  confirmed?
- **Presentation**: Was the presentation format used appropriately? Were the
  visual aids appropriate?

Typical description of mark:

- Below 40%

  Difficult to read and lacks a logical train of thought or argument. Very poor organisation and communication of work.

- Between 40 and 49%

  Poor style of writing with some parts difficult to follow. Poor organisation and presentation of material.

- Between 50 and 59%

  Satisfactorily written and presented with adequate technical content.

- Between 60 and 69%

  Well organised and clearly written with sound technical content. Results analysed and clearly presented.

- Above 70%

  Very well organised and clearly written with good technical content. Results assessed critically and arguments very well presented and supported.

### Scope (50%)

- **Documentation**: Does the documentation have a Tutorial, How to section,
  Reference and Explanation section? Is it clear? Is the source code clear?
- **Modularity**: Is the code written in a modular way?
- **Testing**: Have tests been written for all functionality of the software?

Typical description of mark:

- Below 40%

  The work does not correspond to the project description.

- Between 40 and 49%

  The library includes documentation, some modular functionality and an attempt
  at automatic testing.

- Between 50 and 59%

  The documentation follows the Diataxis framework although it is poorly
  written. The code is modular but has a number of areas of improvement. The
  tests have been written and test some functionality of the code.

- Between 60 and 69%

  The documentation is clear. The code is written in a modular way with few
  areas of improvement. The tests confirm most functionality of the code and the
  documentation.

- Above 70%

  Well written documentation, code is modular and follows all conventions and
  guidelines covered in the course. The tests cover all functionality of the
  code and the documentation.

### Research (20%)

- **State of the field**: Do the authors describe how this software compares
  to other commonly used packages?
- **References**: Is the list of references complete, and is everything
  cited appropriately that should be cited (e.g., papers, datasets,
  software)? Do references in the text use the proper citation syntax?

Typical description of mark:

- Below 40%

  No state if the field or references included.

- Between 40 and 49%

  An inaccurate state of the field included. Some poor references
  included.

- Between 50 and 59%

  An accurate state of the field included. Good references included but with
  little to no context and/or explanation.

- Between 60 and 69%

  A well written state of the field and good references with context and depth.

- Above 70%

  Outstanding state of the field demonstrating a great understanding not only of
  the library but of the already existing tools. The references are all of high
  quality and a thorough demonstration of understanding is given.

Note that this assessment has some overlap with the review criteria
for the Journal of Open Source Software
<https://joss.readthedocs.io/en/latest/review_checklist.html>. Some examples
of papers written for that journal that can be helpful are:

- Matching: A Python library for solving matching games <https://joss.theoj.org/papers/10.21105/joss.02169>
- Nashpy: A Python library for the computation of Nash equilibria <https://joss.theoj.org/papers/10.21105/joss.00904>

Deadline: TBD.

## [Use of Code Generation Tools](#use-of-code-generation-tools)


Code generation tools (for example Large Language Models such as ChatGPT or
Google Gemini) are increasingly capable of producing working programming code.
These tools can sometimes generate high-quality solutions, but they can also
produce incorrect, inefficient, or unsafe code.

**The purpose of this module is for you to develop your own programming,
problem-solving, and debugging skills.**

For this coursework, you must submit your own original work.

You must not:

- Submit code that you do not fully understand
- Submit code generated wholly or partially by automated code generation tools
- Use code generation tools to complete assessed programming tasks

You may:

- Use lecture materials, textbooks, and official documentation
- Discuss general programming ideas with others (but not share code)
- Use standard debugging tools

Automated code generation often produces recognisable artefacts. Where there is
reasonable evidence that submitted work was generated using such tools, the
submission may be treated as an academic integrity violation and penalties may
apply.

**This policy exists to ensure you develop the skills required for later modules
and professional programming practice.**

## [Past group projects](#past-group-projects)

A list of titles of past projects:

{% for year in site.data.projects %}

### {{ year.year }}

{% for title in year.titles %}

- {{ title }}
  {% endfor %}
  {% endfor %}

## [Log of past relevant classes](#log-of-past-relevant-classes)

{% for post in site.posts %}
{% if post.tags contains page.tag %}
[{{post.date | date: "%D"}}: {{ post.title }}]({{site.baseurl}}{{post.url}})
{{ post.excerpt }}
{% endif %}
{% endfor %}
